#+TITLE:    Rationalizability
#+AUTHOR:    Christoph
#+EMAIL:    
#+DATE:      2015-08-07 Fri
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t 
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc 
#+INFOJS_OPT: view:nil toc:nil ltoc:nil mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+HTML_HEAD: <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script>


* Iterative elimination of strictly dominated strategies in Cournot

Here we want to repeat the iterative elimination of strictly dominated actions in the Cournot game. We will use a simple two firm Cournot game with inverse demand

$$p(q_1,q_2)=\max(1-q_1-q_2,0)$$

and marginal costs equal to $c\in(0,1)$. The action set of each player is $[0,1]$. This means that the best response functions are
$$br_1(q_2)=\max\left(\frac{1-q_2-c}{2},0\right)$$
$$br_2(q_1)=\max\left(\frac{1-q_1-c}{2},0\right).$$

We will now eliminate in the first round all actions of player 1 that cannot be a best response for any belief P1 might have. If P1 believes that P2 uses the mixed strategy given by the probability distribution $F(q_2)$, his expected profits are
$$\pi(q_1)= \int_0^1q_1(\max(1-q_1-q_2,0)-c)\,dF(q_2).$$
From the first order condition, we get the generalized best response
$$q_1(F(q_2))=\max\left(\frac{\int_0^11-q_2-c\;dF(q_2)}{2},0\right).$$
If $F$ puts all probability mass on 1, this best response is 0. If $F$ puts all probability mass on 0, the best response is $(1-c)/2$. For all other distributions, the best response is between 0 and $(1-c)/2$. Hence, we can eliminate all quantities greater than $(1-c)/2$: There is no belief such that these quantities would be a best response. 

In the next iteration, we ask which actions can be best responses by P2 given that P1 uses only actions in $[0,(1-c)/2]$. Recall that $br_2(0)=(1-c)/2$ and $br_2((1-c)/2)=(1-c)/4$. For any strategy $F(q_1)$ with support in $[0,(1-c)/2]$, P2's best response in somewhere between $(1-c)/4$ and $(1-c)/2$ (verify!).

Now we can go back to P1...

Computers are better at iterations than humans; so let's feed this problem into the machine. For simplicity, we use $c=0$ although this makes the example formally incorrect: For $c=0$, any quantity $q_i\geq0$ is a best response to $q_j=1$ and therefore the best response function above and the elimination are incorrect. For this example, you should therefore think of $c=0$ as the limit case where $c=\epsilon>0$ and $\epsilon\rightarrow0$. For any $c>0$, the only best response to $q_j=1$ is $0$ and we are in the clear.

#+BEGIN_SRC python :session Cournot :exports both :results output

  import numpy as np
  from matplotlib import pyplot as plt
  from matplotlib import animation
  from prettytable import PrettyTable

  #best response function
  def br(q):
      return (1-q)/2.

  br_vec = np.vectorize(br)

  #inverse of best response function
  def br_inv(q):
      return 1-2*q

  br_inv_vec = np.vectorize(br_inv)

  #calculate the range of best response if the other plays qj Qj
  #starting from Qj=[0,1], iterate this i times
  def iterative_br(i):
      lower,upper = 0.,1.
      for i in xrange(i):
          lower,upper = br(upper),br(lower)
      return lower, upper

  t = PrettyTable(['iteration','lower bound','upper bound'])

  for i in range(0,11,1):
      dummy = iterative_br(i)
      t.add_row([i,round(dummy[0],3),round(dummy[1],3)])

  print t
#+END_SRC

#+RESULTS:
#+begin_example

>>> >>> >>> >>> >>> ... ... ... >>> >>> >>> ... ... ... >>> >>> >>> ... ... ... ... ... ... ... >>> >>> >>> ... ... ... >>> +-----------+-------------+-------------+
| iteration | lower bound | upper bound |
+-----------+-------------+-------------+
|     0     |     0.0     |     1.0     |
|     1     |     0.0     |     0.5     |
|     2     |     0.25    |     0.5     |
|     3     |     0.25    |    0.375    |
|     4     |    0.313    |    0.375    |
|     5     |    0.313    |    0.344    |
|     6     |    0.328    |    0.344    |
|     7     |    0.328    |    0.336    |
|     8     |    0.332    |    0.336    |
|     9     |    0.332    |    0.334    |
|     10    |    0.333    |    0.334    |
+-----------+-------------+-------------+
#+end_example

As you see, we get pretty quickly pretty close to the Nash equilibrium and indeed in the limit only the Nash equilibrium $(1/3,1/3)$ remains. That is, the Nash equilibrium action is the only rationalizable action of the simple Cournot game! The following visualizes this.

#+BEGIN_SRC python :session Cournot :exports None :results file
  fig = plt.figure()
  ax = plt.axes(xlim=(0,1), ylim=(0,1))
  plt.ylabel('$q_1$')
  plt.xlabel('$q_2$')
  line1, = ax.plot([],[],'b',linewidth=1.5)#not yet eliminated q1 on best response function
  line2, = ax.plot([],[],'r',linewidth=1.5)
  ratio1, = ax.plot([],[],'b',linewidth=3)#marks not yet eliminated q1 on axis
  ratio2, = ax.plot([],[],'r',linewidth=3)
  br1,   = ax.plot([],[],'b:',linewidth=1.5)#best response function of P1
  br2,   = ax.plot([],[],'r:',linewidth=1.5)
  plt.legend([br1,br2,ratio1,ratio2],['$br_1(q_2)$','$br_2(q_1)$','not yet eliminated $q_1$','not yet eliminated $q_1$'])

  #basic plot that is held fixed through out all iterations
  def init():
      line1.set_data([],[])
      line2.set_data([],[])
      ratio1.set_data([],[])
      ratio2.set_data([],[])
      x = np.linspace(0,1,50)
      br_curve = br_vec(x)#np.ndarray(br_vec(x))
      br1.set_data(x,br_curve)
      br2.set_data(br_curve,x)
      return line1,line2,br1,br2,ratio1,ratio2

  def animate(i):
      if i % 2 != 0:#true if i is odd
          temp = iterative_br(i)
          y1 = np.linspace(temp[0],temp[1],50)
          x1 = br_inv_vec(y1)
          temp = iterative_br(i-1)
          x2 = np.linspace(temp[0],temp[1],50)
          y2 = br_inv_vec(x2)
      else:#if i is even
          temp = iterative_br(i)
          x2 = np.linspace(temp[0],temp[1],50)
          y2 = br_inv_vec(x2)
          temp = iterative_br(i-1)
          y1 = np.linspace(temp[0],temp[1],50)
          x1 = br_inv_vec(y1)
      helpratio = np.zeros(50)+0.002
      line1.set_data(x1,y1)
      ratio1.set_data(helpratio,y1)
      line2.set_data(x2,y2)
      ratio2.set_data(x2,helpratio)
      return line1,line2,ratio1,ratio2
      
  anim = animation.FuncAnimation(fig,animate,init_func=init,frames=10,repeat=False,interval = 1000,blit=True)

  anim.save('Cournot_iterative.mp4',fps=1,extra_args=['-vcodec','libx264'])

  #plt.show()
  return 'Cournot_iterative.mp4'
#+END_SRC

#+RESULTS:
[[./Cournot_iterative.mp4]]


* Calculating the set of rationalizable actions in finite, two-player games
Here we want to write a small program that calculates the set of rationalizable actions in a finite two player game. We use the following two results mentioned in the lecture:

- the set of rationalizable actions is the set of actions surviving iterative deletion of strictly dominated actions
- an action is strictly dominated if and only if it is a never-best-response.

We will therefore iteratively delete strictly dominated actions. We determine that an action is strictly dominated by establishing that it is a never-best-response. That is, $a$ is strictly dominate if there is no  belief with support in the remaining actions of the other player such that $a$ is a best response given this belief. If we denote the set of actions of player $-i$ that are not deleted (yet) as $\tilde A_{-i}$, then $a$ is strictly dominate if there is no belief $\mu$ on $\tilde A_{-i}$ solving the following system of inequalities

$$\sum_{a_{-i}\in \tilde A_{-i}} u_i(a_i,a_{-i}) \mu(a_{-i})\geq  \sum_{a_{-i}\in \tilde A_{-i}} u_i(a_i',a_{-i}) \mu(a_{-i}) \qquad \text{ for all }a_i'\in \tilde A_i.$$

This system is equivalent to 

$$\sum_{a_{-i}\in \tilde A_{-i}} (u_i(a_i,a_{-i})-u_i(a_i',a_{-i})) \mu(a_{-i})\leq 0  \qquad \text{ for all }a_i'\in \tilde A_i.$$

This is a simple system of linear inequalities and linear program solvers are able to check whether it has a solution or not. So we will start with the first action of P1 and ask a linear program solver whether these inequalities have a solution $\mu$. If not, we remove this action from the game. After that we do the same with the second action of P1 and so on. When we have gone through all actions of P1, we move on to P2. If either for P1 or for P2 an action was removed, we start again, i.e. we go back to P1 and start with his first remaining action (this is the next "iteration" in "iterative deletion of strictly dominated actions"). Then P2... We stop when in one iteration neither an action of P1 nor an action of P2 was removed.

I use the following example game in the code below:

#+CAPTION: Example game
#+ATTR_HTML: :border 2 :rules all :frame border :align center
|   | L    | C    | R    |
|---+------+------+------|
| U | 0,0  | 3,1  | 5,-1 |
| M | 1,3  | 1,1  | 1,-1 |
| B | -1,1 | -2,0 | 2,2  |

#+BEGIN_SRC python :exports both :results output
  """" We give a game table (called "payoffs"; see below) to this program. 
  It then returns the same game but with all non-rationalizable actions removed."""
  from prettytable import PrettyTable
  from openopt import LP
  from operator import *
  #these are the payoffs from the game table; each row in the game table is a list of payoff tuples; 
  #the game table is then a list of these rows
  payoffs = [[(0.,0.),(3.,1.),(5.,-1.)],[(1.,3.),(1.,1.),(1.,-1.)],[(-1.,1.),(-2.,0.),(2.,2.)]]


  #constructs payoffs of only player 1 from the payoffs in "game"
  def payoff1_builder(game):
      payoff1 = []
      for i in game:
          temp = []
          for j in i:
              temp.append(j[0])
          payoff1.append(temp)
      return payoff1

  #constructs payoffs of only player 1 from the payoffs in "game"
  def payoff2_builder(game):
      payoff2 = []
      for i in payoffs:
          temp = []
          for j in i:
              temp.append(j[1])
          payoff2.append(temp)
      return payoff2


  #this function gets a game and removes all rows from the game that correspond to dominated actions of P1; 
  #the game without these rows is returned
  def delete_dom_act_P1(game):
      payoff1 = payoff1_builder(game)
      temp_payoff1 = payoff1
      temp_game = game
      f = [0.]*len(payoff1[0])#dummy objective used below
      lb = [0.]*len(payoff1[0])
      ub = [1.]*len(payoff1[0])
      Aeq = [[1.]*len(payoff1[0])]
      beq = (1.,)
      j = 0
      while j<len(payoff1):
          action = payoff1[j]
          A = []
          b = []
          for other_action in payoff1:
              A.append(map(sub, other_action, action))#elementwise difference
              b.append(0.)
          p = LP(f, A=A,b=b,lb=lb,ub=ub,Aeq=Aeq,beq=beq)#we use the artificial minimization problem under the constraint that action gives a weakly higher payoff than any other action; if no feasible solution is obtained than action is dominated
          p.iprint = -1
          r = p.minimize('pclp')
          if r.stopcase!=1:#if no feasible solution was obtained then action is dominated...
              temp_game.remove(game[j])#...and therefore the row corresponding to this action is removed
          j+=1
      return temp_game

  #this function takes a game and reverses the roles of the players, 
  #i.e. the returned game is the same as the given one but now P1 is column player and P2 is row player
  def interchange_players(game):
      changed_payoff = []
      for row in game:#this loop switches every payoff tuple in the game matrix
          changed_row = []
          for payoff_pair in row:
              n,m = payoff_pair
              changed_row.append((m,n))
          changed_payoff.append(changed_row)
      players_interchanged = [list(x) for x in zip(*changed_payoff)]#transposes the payoff matrix such that P2's actions are in rows instead of columns; 
      return players_interchanged

  #deletes dominated actions of P2 in game; 
  #we do this by interchanging the roles of P1 and P2 and feeding this "switched game" 
  #into the function that deletes dominated actions for P1
  def delete_dom_act_P2(game):
      switched_game = interchange_players(game)
      switched_game_without_dom_act = delete_dom_act_P1(switched_game)
      switched_back_game = interchange_players(switched_game_without_dom_act)
      return switched_back_game
      

  flag = 0#will be zero as long as not all dominated actions are removed yet

  while flag==0:
      temp1 = delete_dom_act_P1(payoffs)
      temp2 = delete_dom_act_P2(temp1)
      if temp2 == payoffs:
          flag = 1
      else:
          payoffs = temp2

  print 'game table without non-rationalizable actions is'
  t = PrettyTable(payoffs[0])
  j = 1
  while j<len(payoffs):
      t.add_row(payoffs[j])
      j+=1

  print t

#+END_SRC

#+RESULTS:

* Iterative elimination of strictly dominated strategies for finite n-player games

We generalize the code above for $n\geq 2$ players. Although it might be a bit counter-intuitive for 2-player games, we will use a slightly different notation for the payoffs: They are now numpy arrays and for two player games we go column by column (instead of row by row). A $n+1$ player game is then simply a numpy array where each element is an $n$ player game (and corresponds to a fixed action of player $n+1$).

The example used here is the following where each player has a single rationalizable action.
#+CAPTION: P3 plays *0*
#+ATTR_HTML: :border 2 :rules all :frame border :align center
|     | 0      | 1     | 2      |
|-----+--------+-------+--------|
| *0* | 1,0,1  | 4,1,2 | 1,-1,1 |
| *1* | 0,3,2  | 1,1,1 | 4,-1,1 |
| *2* | -1,1,0 | 2,0,0 | 2,2,0  |

#+CAPTION: P3 plays *1*
#+ATTR_HTML: :border 2 :rules all :frame border :align center
|     | 0      | 1      | 2      |
|-----+--------+--------+--------|
| *0* | 0,0,0  | 3,1,0  | 5,-1,0 |
| *1* | 1,3,1  | 1,1,0  | 1,-1,0 |
| *2* | -1,1,3 | -2,0,2 | 2,2,3  |


#+BEGIN_SRC python :exports both :results output :tangle yes
  """" We give a game table (called "payoffs"; see below) to this program. 
  It then returns the same game but with all non-rationalizable actions removed."""

  from openopt import LP
  import numpy as np
  from operator import *
  #2player game from above
  #payoffs = np.array([[[0.,0.],[1.,3.],[-1.,1]],[[3.,1.],[1.,1.],[-2.,-0.]],[[5.,-1.],[1.,-1.],[2.,2.]]])
  #payoffs = np.array([[[0.,0.],[1.,3.]],[[1.,1.],[2.,1.]]])
  #3player game (3*3*2 action) in which only (0,1,0) is rationalizable
  payoffs = np.array([[[[1.,0.,1.],[0.,3.,2.],[-1.,1,0.]],[[4.,1.,2.],[1.,1.,1.],[2.,0.,0.]],[[1.,-1.,1.],[4.,-1.,1.],[2.,2.,0.]]],[[[0.,0.,0.],[1.,3.,1.],[-1.,1,3.]],[[3.,1.,0.],[1.,1.,0.],[-2.,-0.,2.]],[[5.,-1.,0.],[1.,-1.,0.],[2.,2.,3.]]]])




  dim = payoffs.shape

  n = payoffs.ndim-1#number of players
  no_a = [] #will contain number of actions available for each player
  #as the first number is number of actions of last player etc., we eventually turn the list around
  for i in dim[:-1]:
      no_a.append(i)
  no_a = no_a[::-1]

  #constructs payoffs of only player i when playing ai from the payoffs in "game", returns a "flat" vector
  def payoffi_builder(game,ai,i):
      j = n-1#the next few lines define a list of slice object to extract i's payoffs when he plays ai from the payoff matrix; 
        #recall that the first index denotes the action of the last player in payoffs!
      obj = ()
      while j>i:
          obj = obj + (slice(0,None,1),)
          j = j-1
      obj = obj + (slice(ai,ai+1,2),)
      j = j-1
      while j>=0:
          obj = obj + (slice(0,None ,1),)
          j = j-1
      obj = obj + (slice(i,i+1,2),)
      payoffi = game[obj]
      return payoffi.flatten()


  #this function gets a game and removes all actions from the game that correspond to dominated actions of Pi; 
  #the game without these actions is returned
  def delete_dom_act_Pi(game,i):
      no_a_game = list(game.shape) #will contain number of actions available for each player
      no_a_game.pop()#removes last element of list which simply was n: the number of payoffs in each action profile
      #as the first number is number of actions of last player etc., we turn the list around
      no_a_game = no_a_game[::-1]
      no_a_i = no_a_game.pop(i)#remove the number of actions of player i from no_a_game and put it in no_a_i
      if no_a_i==1:#if a single action remains it cannot be dominated
          return game
      var_no = np.prod(no_a_game)#size of the support of mu
      if var_no == 1:#special case: the support of mu has a single elment
          return del_dom_a_Pi_point_belief(game, i,no_a_i)
      temp_game = game
      f = [0.]*var_no   #dummy objective used below
      lb = [0.]*var_no
      ub = [1.]*var_no
      Aeq = [[1.]*var_no]
      beq = (1.,)
      j = 0
      while j<no_a_i:
          u_action = payoffi_builder(game,j,i)
          A = []
          b = []
          k = 0
          while k<no_a_i:
              u_other_action = payoffi_builder(game,k,i)
              A.append(u_other_action - u_action)#elementwise difference
              b.append(0.)
              k+=1
          p = LP(f, A=A,b=b,lb=lb,ub=ub,Aeq=Aeq,beq=beq)#we use the artificial minimization problem under the constraint that action gives a weakly higher payoff than any other action; if no feasible solution is obtained than action is dominated
          p.iprint = -1
          r = p.minimize('pclp')
          if r.stopcase!=1:#if no feasible solution was obtained then action is dominated...
              temp_game = np.delete(temp_game,j,n-i-1)#...and therefore action j is removed; recall that players are "in the wrong order"
              count = 0
              z = -1
              while count<=j:
                  z+=1
                  if undominated[i][z]!=-1 :
                      count+=1 
              undominated[i][z] = -1
          j+=1
      return temp_game


  def del_dom_a_Pi_point_belief(game, i,no_i_a):
      to_delete = []
      U=[]
      for action in range(no_i_a):
          U.append(payoffi_builder(game,action,i))
      Umax = max(U)
      for action in range(no_i_a):
          if U[action]!=Umax:
              to_delete.append(action)
              count = 0
              z = -1
              while count<=action:
                  z+=1
                  if undominated[i][z]!=-1 :
                      count+=1 
              undominated[i][z] = -1
      for action in to_delete[::-1]:
          game = np.delete(game,action,n-i-1)
      return game


  flag = 0#will be zero as long as not all dominated actions are removed yet

  ##will contain undominated actions
  undominated = [range(item) for item in no_a]

  temp1 = payoffs
  while flag==0:
      temp2 = temp1
      for k in range(n):
          temp1 = delete_dom_act_Pi(temp1,k)
      if temp2.shape == temp1.shape:
          flag = 1


  print 'game table without non-rationalizable actions is', temp1


  for k in range(n):
      undominated[k] = [item for item in undominated[k] if item!=-1]
      print 'rationalizable actions of player',k,'are', undominated[k]
#+END_SRC
